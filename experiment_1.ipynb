{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63477090",
   "metadata": {},
   "source": [
    "### Experiment1: Resume Parsing -  MistralOCR + SLM/LLM (OpenRouter) + LLM_as_a_judge \n",
    "- Maintainer : Shivargha Bandopadhyay\n",
    "- Date : 10/02/2026\n",
    "- Modules : MistralOCR, Langchain  \n",
    "- Pipeline : MistralOCR -> LLM (Ministral 14B) -> LLM_as_a_judge (Phi4 14B) -> Loop(on Failure, to MistralOCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "696fe5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from mistralai import Mistral \n",
    "from langchain_core.messages import HumanMessage,SystemMessage,AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate,SystemMessagePromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List,Annotated, Literal,TypedDict,Optional\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "975e8dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OCRResponse(pages=[OCRPageObject(index=0, markdown='Harsh Raj\\n\\nNoida, Uttar Pradesh, India | rajharsh5450@gmail.com | (+91) 8340643631 | LinkedIn\\n\\n# EXPERIENCE\\n\\nData Engineer 1, MAQ Software - Noida, Uttar Pradesh, India\\nJuly 2023 - Present\\n\\n- Design, Build, and Optimise Data Pipelines: Implemented a Medallion Architecture in Azure Databricks for a Sales and Revenue Organisation, migrating and processing data from SQL Server and on-premises CSV files. Designed and optimised pipelines by focussing on data quality, lineage, and scalability, enabling integration of over 80,000 sales-related records daily.\\n- Modernised ETL Architecture: Redesigned and optimized a Microsoft Fabric architecture for an ISV client to process 500 GB of data reducing their end-to-end data refresh duration to 70 minutes from 4 hours. Enabled end-to-end ingestion, processing, and reporting across 3 workspaces, improving scaling and data quality.\\n- CI/CD Pipeline for Microsoft Fabric Deployment: Developed a CI/CD pipeline in Azure DevOps to deploy Microsoft Fabric artifacts via APIs, Azure Key Vault, user management, and operations logging. Achieved an end-to-end deployments in under 7 minutes, reducing manual efforts by 60% and improving release consistency across 3 workspaces.\\n- Operations Dashboard: Created a real-time dashboard to monitor 10+ pipeline metrics, improving visibility and system health tracking. Reduced issue resolution time by 40%, enhancing reliability and stakeholder responsiveness.\\n- Initiative: Led organization-wide Microsoft Fabric certification training, enabling 1,000+ employees to prepare for and pass the DP-600 and DP-700 certifications. Composed 4+ utilities to streamline development without compromising quality, including a Fabric shortcut creation tool.\\n\\nAssociate Software Engineer, MAQ Software - Noida, Uttar Pradesh, India\\nJan 2023 - July 2023\\n\\n- Self-Service Analytics and Data Modeling: Prepared semantic data models, defined entity relationships, and authored DAX measures to support self-service analytics, reducing manual reporting workload by 60%.\\n- Executive Dashboards and Visualization: Designed interactive, executive-level Power BI dashboards that accelerated decision-making through real-time KPIs.\\n- Optimization and Performance: Migrated large-scale SQL queries to KQL, improving responsiveness and reducing development cycles by 40%.\\n- Automation and Governance: Automated Dynamics workflows using custom plugins, reducing repetitive operational efforts by 60%. Implemented role based access controls in model-driven apps to strengthen data security and compliance.\\n\\n# TECHNICAL SKILLS\\n\\n- Languages: Python, SQL, KQL, C/C++\\n- Cloud and Big Data Technologies: Azure Data Factory, Azure Databricks, Microsoft Fabric, Azure Synapse Analytics, Azure Data Lake Storage Gen2 (ADLS), Apache Kafka, Apache Spark\\n- Data Engineering: PySpark, Pandas, Spark SQL, ETL/ELT Pipelines, Medallion Architecture, Star/Snowflake Schema Design\\n- Analytics and Reporting: Power BI, DAX, Data Modeling, Semantic Modeling, KPI Dashboards\\n- Tools: GitHub, Azure DevOps, SSMS, VS Code, Visual Studio\\n\\n# ACHIEVEMENTS\\n\\n- Microsoft Certified: Fabric Data Engineer Associate (DP-700)\\n- Microsoft Certified: Fabric Analytics Engineer Associate (DP-600)\\n\\n# EDUCATION\\n\\nIndian Institute of Information Technology, Kalyani, Bachelor of Technology in CSE\\nJuly 2019 - Jun 2023\\nGPA: 8.85/10\\n\\nPt.B.P.Saraswati Vidya Mandir, Jharkhand, India, CBSE Intermediate Examination\\nMar 2017 - Mar 2019\\nPercentage: 86.6%\\n\\n# PROJECTS\\n\\n- Sales Analytics Platform: Built an end-to-end data analytics platform that consolidated multi-source sales data into Azure Data Lake using Azure Data Factory and Fabric Notebooks (PySpark). Implemented a Delta-based Medallion Architecture to improve pipeline reliability and reduce processing time by 40%. Automated ingestion and orchestration through parameterized ADF pipelines and Azure DevOps, eliminating most manual effort. Delivered Power BI dashboards providing real-time sales and customer insights, accelerating reporting cycles by 70%.\\n- Data Pipeline Utilities: Developed reusable pipeline utilities to standardize transformation logic, improve maintainability, and accelerate data processing across multiple projects.', images=[], dimensions=OCRPageDimensions(dpi=200, height=2339, width=1654), tables=[], hyperlinks=['mailto:rajharsh5450%40gmail.com', 'https://www.linkedin.com/in/rajharsh5450/', 'https://learn.microsoft.com/en-us/users/harshraj-7992/credentials/8633b9905ccdc56?ref=https%3A%2F%2Fwww.linkedin.com%2F', 'https://learn.microsoft.com/en-us/users/harshraj-7992/credentials/14547d4b9f39044f?ref=https%3A%2F%2Fwww.linkedin.com%2F'], header=None, footer=None)], model='mistral-ocr-latest', usage_info=OCRUsageInfo(pages_processed=1, doc_size_bytes=59147), document_annotation=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MISTRAL OCR ###\n",
    "'''\n",
    "Local Files Need to be uploaded to Mistral OCR - storage\n",
    "Use Mistral OCR API to extract text from the uploaded file URL Link\n",
    "'''\n",
    "client = Mistral(api_key=MISTRAL_API_KEY)\n",
    "def upload_file(file_path:str) -> str:\n",
    "\n",
    "    filename = file_path.split(\"\\\\\")[-1]\n",
    "\n",
    "    ## Upload File to Mistral OCR Storage ##\n",
    "    uploaded_file = client.files.upload(\n",
    "        file = {\n",
    "            \"file_name\": filename,\n",
    "            \"content\": open(file_path, \"rb\")\n",
    "        },\n",
    "        purpose = 'ocr'\n",
    "    )\n",
    "\n",
    "    signed_url = client.files.get_signed_url(file_id=uploaded_file.id)\n",
    "    return signed_url.url\n",
    "\n",
    "def get_ocr_response(file_url:str):\n",
    "\n",
    "    ## Get OCR Response ##\n",
    "    ocr_response = client.ocr.process(\n",
    "        model = 'mistral-ocr-latest',\n",
    "        document = {\n",
    "            \"type\": \"document_url\",\n",
    "            \"document_url\":file_url\n",
    "        },\n",
    "        include_image_base64 = True    \n",
    "    )\n",
    "\n",
    "    return ocr_response\n",
    "\n",
    "url = upload_file(\"D:\\\\Tvarah\\\\resume_extraction\\\\data\\\\DE_HARSHRAJ[3y_0m]_Good Resume.pdf\")\n",
    "ocr_response = get_ocr_response(url)\n",
    "ocr_response \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffea90f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prompt Templates ###\n",
    "system_prompt = SystemMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are an expert AI Resume Parser & Data Extraction Specialist. \n",
    "Your goal is to extract structured candidate data from raw Resume Markdown text with 100% precision.\n",
    "### CORE EXTRACTION RULES:\n",
    "1.  **Truthfulness:** Extract ONLY what is explicitly stated in the text. Do not infer or hallucinate data. If a field (like \"LinkedIn URL\" or \"End Date\") is missing, leave it as null/None. Do not invent \"N/A\" or placeholders.\n",
    "2.  **Date Normalization:** * Convert all dates to `YYYY-MM` format (e.g., \"August 2022\" -> \"2022-08\").\n",
    "    * If a candidate writes \"Present\", \"Current\", or \"Till Date\", set the `end_date` to None.\n",
    "3.  **Name Extraction:**\n",
    "    * If the resume has a header like \"Resume of John Doe\", extract only \"John Doe\".\n",
    "    * Do not include titles like \"Mr.\", \"Dr.\", or suffixes like \"PMP\" in the `full_name` field.\n",
    "4.  **Work Experience:**\n",
    "    * Split \"Role\" and \"Company\" if they appear on the same line (e.g., \"Software Engineer | Google\" -> Role: \"Software Engineer\", Company: \"Google\").\n",
    "    * If a description contains \"References available upon request\", ignore that line.\n",
    "\n",
    "### HANDLING EDGE CASES:\n",
    "1. **Multi-Role:** If a candidate held multiple roles at the same company, treat them as separate objects in `work_experience`.\n",
    "2. **Education:** Every new university/college in the education section should be treated as a separate object in `education`.\n",
    "3. If you dont find any data for a field, leave it as None.\n",
    "4. For City, Country, State - Analyze based on the current work location, if current location comes out remote,\\\n",
    "    try to find out current location, current country, current state from previous work experiences, if not found then return None for all three.\n",
    "\n",
    "You will receive the Resume Markdown below. Populate the data model accurately.\n",
    "\"\"\"   \n",
    ")\n",
    "\n",
    "human_prompt = HumanMessagePromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Here is the Resume Markdown:\n",
    "    <resume_markdown>\n",
    "    {resume_markdown}\n",
    "    </resume_markdown>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [system_prompt, human_prompt]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0f25f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pydantic Structures ###\n",
    "\n",
    "## Personal Information Class ##\n",
    "class personalInfo(BaseModel):\n",
    "    full_name: str = Field(...,description=\"Full name of the candidate\")\n",
    "    first_name: str = Field(...,description=\"First name of the candidate\")\n",
    "    middle_name: str|None = Field(default=None,description=\"Middle name of the candidate,if not present then return None\")\n",
    "    last_name: str|None = Field(default=None,description=\"Last name of the candidate, if not present then return None\")\n",
    "    date_of_birth: str|None = Field(default=None,description=\"Date of birth of the candidate, if not present then return None\")\n",
    "    gender: str|None = Field(default=None,description=\"Gender of the candidate, if not present then return None\")\n",
    "    nationality: str|None = Field(default=None,description=\"Nationality of the candidate, if not present then return None\")\n",
    "    work_authorization: str|None = Field(default=None,description=\"Work authorization of the candidate, if not present then return None\")\n",
    "\n",
    "## Contact Information Class ##\n",
    "class contactInfo(BaseModel):\n",
    "    primary_email : str|None = Field(default=None,description=\"Primary email address of the candidate, if not present then return None\")\n",
    "    secondary_email : str|None = Field(default=None,description=\"Secondary email address of the candidate, if not present then return None\")\n",
    "    primary_phone_number : str|None = Field(default=None,description=\"Primary phone number of the candidate, if not present then return None\")\n",
    "    secondary_phone_number : str|None = Field(default=None,description=\"Secondary phone number of the candidate, if not present then return None\")\n",
    "    country_code : str|None = Field(default=None,description=\"Country code of the candidate, if not present then return None\")\n",
    "    current_city : str|None = Field(default=None,description=\"Current city of the candidate, if not present then return None\")\n",
    "    current_state : str|None = Field(default=None,description=\"Current state of the candidate, if not present then return None\")\n",
    "    current_country : str|None = Field(default=None,description=\"Current country of the candidate, if not present then return None\")\n",
    "    postal_address: str|None = Field(default=None,description=\"Address of the candidate, if not present then return None\")\n",
    "\n",
    "### Education class ###\n",
    "class educationInfo(BaseModel):\n",
    "    institution_name: str|None = Field(default=None,description=\"Institution name of the candidate, if not present then return None\")\n",
    "    institution_type: str|None = Field(default=None,description=\"Institution type (University,College,School,Bootcamp,etc), if not present then return None\")\n",
    "    institution_country : str|None = Field(default=None,description=\"Country of the institution, if not present then return None\")\n",
    "    degree: str|None = Field(default=None,description=\"Degree of the candidate with respect to the institution,\\\n",
    "        example: B.E, B.Tech, M.E, M.Tech, Ph.D, etc. if not present then return None\")\n",
    "\n",
    "    field_of_study: str|None = Field(default=None,description=\"Field of study of the candidate with respect to the institution, \\\n",
    "        example: Computer Science, Information Technology, etc. if not present then return None\")\n",
    "    specialisation : str|None = Field(default=None,description=\"Specialisation of the candidate with respect to the institution, \\\n",
    "        example: Artificial Intelligence, Marketing, Finance etc. if not present then return None\")\n",
    "    education_level: str|None = Field(default=None,description=\"Education level of the candidate with respect to the institution, \\\n",
    "        example: high_school / bachelors / masters / phd / diploma/bootcamp/certificate, if not present then return None\")\n",
    "    \n",
    "    start_date: str|None = Field(default=None,description=\"Start date at the institution, if not present then return None\")\n",
    "    end_date: str|None = Field(default=None,description=\"End date of the institution, \\\n",
    "        If mentioned - Present or Ongoing, return None\")\n",
    "    is_current: str|None = Field(default=None,description=\"Yes, if the candidate is currently pursuing the degree, \\\n",
    "        No, if the candidate has completed the degree, if not present then return None\")\n",
    "    \n",
    "    grade_or_gpa: str|None = Field(default=None,description=\"Grade or GPA of the candidate with respect to the institution, \\\n",
    "        if not present then return None\")\n",
    "    mode : str|None = Field(default=None,description=\"Mode of the institution, \\\n",
    "        example: full_time, part_time, online, etc. if not present then return None\")\n",
    "\n",
    "\n",
    "### Work experience class ###\n",
    "class workExperienceInfo(BaseModel):\n",
    "    company_name: str = Field(...,description=\"Company name of the candidate\")\n",
    "    company_location: str|None = Field(default=None,description=\"Location of the company, if not present then return None\")\n",
    "    job_title: str|None = Field(default=None,description=\"Job title of the candidate, if not present then return None\")\n",
    "    employment_type: str|None = Field(default=None,description=\"Employment type of the candidate with respect to the company,\\\n",
    "        example: full_time, part_time, contract, temporary,internship etc. if not present then return None\")\n",
    "    \n",
    "    start_date: str|None = Field(default=None,description=\"Start date of the role, if not present then return None\")\n",
    "    end_date: str|None = Field(default=None,description=\"End date of the role, if not present then return None\")\n",
    "    is_current_role: str|None = Field(default=None,description=\"Yes, if the candidate is currently working in the role, if not present then return None\")\n",
    "    role_description: str = Field(...,description=\"Role description of the role, \\\n",
    "        This should be a brief summary of the role and responsibilities,not more than 200words.\")\n",
    "    \n",
    "    #team_or_department: str|None = Field(default=None,description=\"Team or department of the role, if not present then return None\")\n",
    "    #tools_mentioned: list[str] = Field(default=[],description=\"Tools the candidate has worked on with respect to the role,\\\n",
    "    #Ex:  if not present then return None\")\n",
    "    #technologies_mentioned: list[str] = Field(default=[],description=\"Technologies the candidate has worked on with respect to the role.\\\n",
    "    # if not present then return None\")\n",
    "\n",
    "class skillsInfo(BaseModel):\n",
    "    #technical_skills: list[str] = Field(default=[],description=\"Technical skills of the candidate, if not present then return None\")\n",
    "    programming_languages: list[str] = Field(default=[],description=\"Programming languages the candidate has worked on,\\\n",
    "        Ex: Python,C,C++ etc if not present then return None\")\n",
    "    frameworks_and_libraries: list[str] = Field(default=[],description=\"Frameworks and libraries the candidate has worked on, \\\n",
    "        Ex: React,Angular,Django,Flask,Pandas,Pytorch etc if not present then return None\")\n",
    "    tools_and_platforms: list[str] = Field(default=[],description=\"Tools and platforms the candidate has worked on,\\\n",
    "        Ex: Git,Docker,Kubernetes,Jenkins,Jira,etc. if not present then return None\")\n",
    "    databases: list[str] = Field(default=[],description=\"Databases the candidate has worked on, \\\n",
    "        Ex: MySQL,PostgreSQL,MongoDB,Oracle,etc. if not present then return None\")\n",
    "    cloud_and_infra: list[str] = Field(default=[],description=\"Cloud and infra the candidate has worked on,\\\n",
    "        Ex: AWS,Azure,GCP,etc. if not present then return None\")\n",
    "    soft_skills: list[str] = Field(default=[],description=\"Soft skills of the candidate, \\\n",
    "        Ex: Communication,Teamwork,Leadership,Problem-solving,etc. if not present then return None\")\n",
    "    domain_skills: list[str] = Field(default=[],description=\"Domain skills of the candidate, \\\n",
    "        Ex: Finance,Healthcare,Education,etc. if not present then return None\")\n",
    "    certified_skills: list[str] = Field(default=[],description=\"Certified skills of the candidate, if not present then return None\")\n",
    "    \n",
    "class ResumeData(BaseModel):\n",
    "    personal_info: personalInfo = Field(...,description=\"Personal information of the candidate\")\n",
    "    contact_info: contactInfo = Field(...,description=\"Contact information of the candidate\")\n",
    "    education_info: list[educationInfo] = Field(...,description=\"Education information of the candidate\")\n",
    "    work_experience_info: list[workExperienceInfo] = Field(...,description=\"Work experience information of the candidate\")\n",
    "    skills_info: skillsInfo = Field(...,description=\"Skills information of the candidate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "984467ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResumeData(personal_info=personalInfo(full_name='Harsh Raj', first_name='Harsh', middle_name=None, last_name='Raj', date_of_birth=None, gender=None, nationality=None, work_authorization=None), contact_info=contactInfo(primary_email='rajharsh5450@gmail.com', secondary_email=None, primary_phone_number='+918340643631', secondary_phone_number=None, country_code='IN', current_city='Noida', current_state='Uttar Pradesh', current_country='India', postal_address=None), education_info=[educationInfo(institution_name='Indian Institute of Information Technology, Kalyani', institution_type='University', institution_country='India', degree='Bachelor of Technology', field_of_study='Computer Science and Engineering', specialisation=None, education_level='bachelors', start_date='2019-07', end_date='2023-06', is_current='No', grade_or_gpa='8.85/10', mode=None), educationInfo(institution_name='Pt.B.P.Saraswati Vidya Mandir, Jharkhand, India', institution_type='School', institution_country='India', degree=None, field_of_study=None, specialisation=None, education_level='high_school', start_date='2017-03', end_date='2019-03', is_current='No', grade_or_gpa='86.6%', mode=None)], work_experience_info=[workExperienceInfo(company_name='MAQ Software', company_location='Noida, Uttar Pradesh, India', job_title='Data Engineer 1', employment_type=None, start_date='2023-07', end_date=None, is_current_role='Yes', role_description='Design, Build, and Optimise Data Pipelines: Implemented a Medallion Architecture in Azure Databricks for a Sales and Revenue Organisation, migrating and processing data from SQL Server and on-premises CSV files. Designed and optimised pipelines by focussing on data quality, lineage, and scalability, enabling integration of over 80,000 sales-related records daily. Modernised ETL Architecture: Redesigned and optimized a Microsoft Fabric architecture for an ISV client to process 500 GB of data, reducing their end-to-end data refresh duration to 70 minutes from 4 hours. Enabled end-to-end ingestion, processing, and reporting across 3 workspaces, improving scaling and data quality. CI/CD Pipeline for Microsoft Fabric Deployment: Developed a CI/CD pipeline in Azure DevOps to deploy Microsoft Fabric artifacts via APIs, Azure Key Vault, user management, and operations logging. Achieved end-to-end deployments in under 7 minutes, reducing manual efforts by 60% and improving release consistency across 3 workspaces. Operations Dashboard: Created a real-time dashboard to monitor 10+ pipeline metrics, improving visibility and system health tracking. Reduced issue resolution time by 40%, enhancing reliability and stakeholder responsiveness. Initiative: Led organization-wide Microsoft Fabric certification training, enabling 1,000+ employees to prepare for and pass the DP-600 and DP-700 certifications. Composed 4+ utilities to streamline development without compromising quality, including a Fabric shortcut creation tool.'), workExperienceInfo(company_name='MAQ Software', company_location='Noida, Uttar Pradesh, India', job_title='Associate Software Engineer', employment_type=None, start_date='2023-01', end_date='2023-07', is_current_role='No', role_description='Self-Service Analytics and Data Modeling: Prepared semantic data models, defined entity relationships, and authored DAX measures to support self-service analytics, reducing manual reporting workload by 60%. Executive Dashboards and Visualization: Designed interactive, executive-level Power BI dashboards that accelerated decision-making through real-time KPIs. Optimization and Performance: Migrated large-scale SQL queries to KQL, improving responsiveness and reducing development cycles by 40%. Automation and Governance: Automated Dynamics workflows using custom plugins, reducing repetitive operational efforts by 60%. Implemented role-based access controls in model-driven apps to strengthen data security and compliance.')], skills_info=skillsInfo(programming_languages=['Python', 'SQL', 'KQL', 'C', 'C++'], frameworks_and_libraries=['PySpark', 'Pandas', 'Spark SQL'], tools_and_platforms=['GitHub', 'Azure DevOps', 'SSMS', 'VS Code', 'Visual Studio', 'Azure Data Factory', 'Azure Databricks', 'Microsoft Fabric', 'Azure Synapse Analytics', 'Azure Data Lake Storage Gen2 (ADLS)', 'Apache Kafka', 'Apache Spark'], databases=[], cloud_and_infra=['Azure Data Factory', 'Azure Databricks', 'Microsoft Fabric', 'Azure Synapse Analytics', 'Azure Data Lake Storage Gen2 (ADLS)', 'Apache Kafka', 'Apache Spark'], soft_skills=[], domain_skills=[], certified_skills=['Microsoft Certified: Fabric Data Engineer Associate (DP-700)', 'Microsoft Certified: Fabric Analytics Engineer Associate (DP-600)']))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model = ChatOpenAI(\n",
    "    base_url = \"https://openrouter.ai/api/v1\",\n",
    "    api_key = OPENROUTER_API_KEY,\n",
    "    model = \"mistralai/ministral-14b-2512\",\n",
    ")\n",
    "chain = prompt_template | llm_model.with_structured_output(ResumeData)\n",
    "response = chain.invoke({\"resume_markdown\": ocr_response.pages[0].markdown})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ebe65731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"personal_info\": {\n",
      "        \"full_name\": \"Harsh Raj\",\n",
      "        \"first_name\": \"Harsh\",\n",
      "        \"middle_name\": null,\n",
      "        \"last_name\": \"Raj\",\n",
      "        \"date_of_birth\": null,\n",
      "        \"gender\": null,\n",
      "        \"nationality\": null,\n",
      "        \"work_authorization\": null\n",
      "    },\n",
      "    \"contact_info\": {\n",
      "        \"primary_email\": \"rajharsh5450@gmail.com\",\n",
      "        \"secondary_email\": null,\n",
      "        \"primary_phone_number\": \"+918340643631\",\n",
      "        \"secondary_phone_number\": null,\n",
      "        \"country_code\": \"IN\",\n",
      "        \"current_city\": \"Noida\",\n",
      "        \"current_state\": \"Uttar Pradesh\",\n",
      "        \"current_country\": \"India\",\n",
      "        \"postal_address\": null\n",
      "    },\n",
      "    \"education_info\": [\n",
      "        {\n",
      "            \"institution_name\": \"Indian Institute of Information Technology, Kalyani\",\n",
      "            \"institution_type\": \"University\",\n",
      "            \"institution_country\": \"India\",\n",
      "            \"degree\": \"Bachelor of Technology\",\n",
      "            \"field_of_study\": \"Computer Science and Engineering\",\n",
      "            \"specialisation\": null,\n",
      "            \"education_level\": \"bachelors\",\n",
      "            \"start_date\": \"2019-07\",\n",
      "            \"end_date\": \"2023-06\",\n",
      "            \"is_current\": \"No\",\n",
      "            \"grade_or_gpa\": \"8.85/10\",\n",
      "            \"mode\": null\n",
      "        },\n",
      "        {\n",
      "            \"institution_name\": \"Pt.B.P.Saraswati Vidya Mandir, Jharkhand, India\",\n",
      "            \"institution_type\": \"School\",\n",
      "            \"institution_country\": \"India\",\n",
      "            \"degree\": null,\n",
      "            \"field_of_study\": null,\n",
      "            \"specialisation\": null,\n",
      "            \"education_level\": \"high_school\",\n",
      "            \"start_date\": \"2017-03\",\n",
      "            \"end_date\": \"2019-03\",\n",
      "            \"is_current\": \"No\",\n",
      "            \"grade_or_gpa\": \"86.6%\",\n",
      "            \"mode\": null\n",
      "        }\n",
      "    ],\n",
      "    \"work_experience_info\": [\n",
      "        {\n",
      "            \"company_name\": \"MAQ Software\",\n",
      "            \"company_location\": \"Noida, Uttar Pradesh, India\",\n",
      "            \"job_title\": \"Data Engineer 1\",\n",
      "            \"employment_type\": null,\n",
      "            \"start_date\": \"2023-07\",\n",
      "            \"end_date\": null,\n",
      "            \"is_current_role\": \"Yes\",\n",
      "            \"role_description\": \"Design, Build, and Optimise Data Pipelines: Implemented a Medallion Architecture in Azure Databricks for a Sales and Revenue Organisation, migrating and processing data from SQL Server and on-premises CSV files. Designed and optimised pipelines by focussing on data quality, lineage, and scalability, enabling integration of over 80,000 sales-related records daily. Modernised ETL Architecture: Redesigned and optimized a Microsoft Fabric architecture for an ISV client to process 500 GB of data, reducing their end-to-end data refresh duration to 70 minutes from 4 hours. Enabled end-to-end ingestion, processing, and reporting across 3 workspaces, improving scaling and data quality. CI/CD Pipeline for Microsoft Fabric Deployment: Developed a CI/CD pipeline in Azure DevOps to deploy Microsoft Fabric artifacts via APIs, Azure Key Vault, user management, and operations logging. Achieved end-to-end deployments in under 7 minutes, reducing manual efforts by 60% and improving release consistency across 3 workspaces. Operations Dashboard: Created a real-time dashboard to monitor 10+ pipeline metrics, improving visibility and system health tracking. Reduced issue resolution time by 40%, enhancing reliability and stakeholder responsiveness. Initiative: Led organization-wide Microsoft Fabric certification training, enabling 1,000+ employees to prepare for and pass the DP-600 and DP-700 certifications. Composed 4+ utilities to streamline development without compromising quality, including a Fabric shortcut creation tool.\"\n",
      "        },\n",
      "        {\n",
      "            \"company_name\": \"MAQ Software\",\n",
      "            \"company_location\": \"Noida, Uttar Pradesh, India\",\n",
      "            \"job_title\": \"Associate Software Engineer\",\n",
      "            \"employment_type\": null,\n",
      "            \"start_date\": \"2023-01\",\n",
      "            \"end_date\": \"2023-07\",\n",
      "            \"is_current_role\": \"No\",\n",
      "            \"role_description\": \"Self-Service Analytics and Data Modeling: Prepared semantic data models, defined entity relationships, and authored DAX measures to support self-service analytics, reducing manual reporting workload by 60%. Executive Dashboards and Visualization: Designed interactive, executive-level Power BI dashboards that accelerated decision-making through real-time KPIs. Optimization and Performance: Migrated large-scale SQL queries to KQL, improving responsiveness and reducing development cycles by 40%. Automation and Governance: Automated Dynamics workflows using custom plugins, reducing repetitive operational efforts by 60%. Implemented role-based access controls in model-driven apps to strengthen data security and compliance.\"\n",
      "        }\n",
      "    ],\n",
      "    \"skills_info\": {\n",
      "        \"programming_languages\": [\n",
      "            \"Python\",\n",
      "            \"SQL\",\n",
      "            \"KQL\",\n",
      "            \"C\",\n",
      "            \"C++\"\n",
      "        ],\n",
      "        \"frameworks_and_libraries\": [\n",
      "            \"PySpark\",\n",
      "            \"Pandas\",\n",
      "            \"Spark SQL\"\n",
      "        ],\n",
      "        \"tools_and_platforms\": [\n",
      "            \"GitHub\",\n",
      "            \"Azure DevOps\",\n",
      "            \"SSMS\",\n",
      "            \"VS Code\",\n",
      "            \"Visual Studio\",\n",
      "            \"Azure Data Factory\",\n",
      "            \"Azure Databricks\",\n",
      "            \"Microsoft Fabric\",\n",
      "            \"Azure Synapse Analytics\",\n",
      "            \"Azure Data Lake Storage Gen2 (ADLS)\",\n",
      "            \"Apache Kafka\",\n",
      "            \"Apache Spark\"\n",
      "        ],\n",
      "        \"databases\": [],\n",
      "        \"cloud_and_infra\": [\n",
      "            \"Azure Data Factory\",\n",
      "            \"Azure Databricks\",\n",
      "            \"Microsoft Fabric\",\n",
      "            \"Azure Synapse Analytics\",\n",
      "            \"Azure Data Lake Storage Gen2 (ADLS)\",\n",
      "            \"Apache Kafka\",\n",
      "            \"Apache Spark\"\n",
      "        ],\n",
      "        \"soft_skills\": [],\n",
      "        \"domain_skills\": [],\n",
      "        \"certified_skills\": [\n",
      "            \"Microsoft Certified: Fabric Data Engineer Associate (DP-700)\",\n",
      "            \"Microsoft Certified: Fabric Analytics Engineer Associate (DP-600)\"\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(response.model_dump_json(indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc20ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
